{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Algorithms_notes.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZEA4wuyaS8z",
        "colab_type": "text"
      },
      "source": [
        "#This is gonna be my notes for ML algorithms\n",
        "#I will test algos on certain parameters as well as we will go through maths also.\n",
        "#I will judge on following parameters.......\n",
        "\n",
        "**1. Regression/Clasification**<br>\n",
        "**2. Basic Working**<br>\n",
        "**3. Maths**<br>\n",
        "**4. Loss function**<br>\n",
        "**5. Overfitting/Underfitting(Hyperparameter)**<br>\n",
        "**6. Performance in High dimension**<br>\n",
        "**7. Performance on Large dataset**<br>\n",
        "**8. Performance on imbalance data**<br>\n",
        "**9. Performance on Missing values**<br>\n",
        "**10. Performance in presence of Outliers**<br>\n",
        "**11. Performance on Numerical/Continuous data**<br>\n",
        "**12. Other factors affect performance**<br>\n",
        "**13. Pros and Cons**<br>\n",
        "\n",
        "**Hurrrah!!!!**\n",
        "\n",
        "**Let's begin** \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUJwhw-iJ5l4",
        "colab_type": "text"
      },
      "source": [
        "# **fghfhfh**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqnIT_6MegUY",
        "colab_type": "text"
      },
      "source": [
        "#**Linear regression**\n",
        "<br>\n",
        "\n",
        "**It's a Regression Algorithm**\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "**Convex Function**\n",
        "\n",
        "* ![](https://image.slidesharecdn.com/dtuphd12-120919100037-phpapp02/95/convex-optimization-old-tricks-for-new-problems-24-728.jpg?cb=1348048958)\n",
        "\n",
        "<br>\n",
        "\n",
        "* ![alt text](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTjS_bLijszlZJ1W1RRtbyxjbu9m-bBUOkBl4XO36kPFwm21CLf)\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "**Linear regression is a statistical method of finding the relationship between independent(target) and dependent(input) variables.The idea behind simple linear regression is to \"fit\" the observations of two variables into a linear relationship between them**\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "![alt text](http://www.picronline.org/articles/2017/8/2/images/PerspectClinRes_2017_8_2_100_203040_f1.jpg)\n",
        "\n",
        "<br>\n",
        "\n",
        "#**Assumptions of Linear Regression**\n",
        "\n",
        "Linear regression is a parametric approach.It has some assumptions regarding data.\n",
        "* Linear and additive relationship.<br>\n",
        "linear->change in Y(output) due to change in change in X(input) is regardless of value of X.<br>\n",
        "Additive->effect of X1 on Y is independent of other input variables\n",
        "* No correlation between Residual(error) terms\n",
        "* Input variables must have constant variance(Homoskedasticity)\n",
        "* Input varibles must not be correlated\n",
        "* Error terms must be normaly disttributed\n",
        " \n",
        "*At the end of this notebook.We will discuss what if these assumptions fail?*\n",
        "#**Mathematics**\n",
        "<br>\n",
        "Equation of a line:\n",
        " y=mx+b\n",
        " \n",
        " m-->slope<br>\n",
        " b-->intercept\n",
        " \n",
        "We can estimate the values of this by using\n",
        "* Ordinary Least Square\n",
        "* Gradient Descent\n",
        "\n",
        "*Sum of squared error(SSE is the sum of the squared differences between each observation and its group's mean. It can be used as a measure of variation within a cluster.It performs badly)*\n",
        "\n",
        "**Ordinary Least Square(OLS)**<br>\n",
        "In order to use OLS we need to use following formula<br>\n",
        "\n",
        "![](https://cdn-images-1.medium.com/max/800/1*ypggCCmwRs4tjXqscH8N4Q.png)\n",
        "![](https://cdn-images-1.medium.com/max/800/1*_wFBlqEDEzOp6Tx3suNj2g.png)<br>\n",
        "*Ordinary Least Square method looks simple and computation is easy. But, this OLS method will work for both univariate dataset which is single independent variables and single dependent variables and multi-variate dataset. Multi-variate dataset contains a single independent variables set and multiple dependent variables sets, require us to use a machine learning algorithm called “Gradient Descent.*\n",
        "\n",
        "**Gradient Descent**\n",
        "<br>\n",
        "Gradient descent algorithm’s main objective is to minimise the cost function. It is one of the best optimisation algorithms to minimise errors (difference of actual value and predicted value).\n",
        "\n",
        "Let’s represent the hypothesis h, which is function or a learning algorithm.<br>\n",
        "![](https://cdn-images-1.medium.com/max/800/1*K1-0bnoMqxSv6tZXpOki1Q.png)\n",
        "\n",
        "The goal is similar like the above operation that we did to find out a best fit of intercept line ‘y’ in the slope ‘m’. Using Gradient descent algorithm also, we will figure out a minimal cost function by applying various parameters for theta 0 and theta 1 and see the slope intercept until it reaches convergence.\n",
        "\n",
        "In a real world example, it is similar to find out a best direction to take a step downhill.<br>\n",
        "\n",
        "![](https://cdn-images-1.medium.com/max/800/1*09kq2L23D9XM_9Xtr8gc8Q.png)\n",
        "<br>\n",
        "\n",
        "We take a step towards the direction to get down. From the each step, you look out the direction again to get down faster and downhill quickly. The similar approach is using in this algorithm to minimise cost function.\n",
        "\n",
        "We can measure the accuracy of our hypothesis function by using a cost function and the formula is<br>\n",
        "![](https://cdn-images-1.medium.com/max/800/1*O0VceUdYADRY3FoTbBrqMA.png)\n",
        "<br>\n",
        " ![](https://cdn-images-1.medium.com/max/800/1*TrFUjJR65rWBe3wN9Snacg.png)\n",
        " \n",
        "Partial derivatives represents the rate of change of the functions as the variable change. In our case we change values for theta 0 and theta 1 and identifies the rate of change. To apply rate of change values for theta 0 and theta 1, the below are the equations for theta 0 and theta 1 to apply it on each epoch\n",
        "![](https://cdn-images-1.medium.com/max/800/1*G3evFxIAlDchOx5Wl7bV5g.png)\n",
        "<br>\n",
        "![](https://cdn-images-1.medium.com/max/800/1*HrFZV7pKPcc5dzLaWvngtQ.png)\n",
        "\n",
        "Here alpha is learning rate.It need to be decided.If alpha is too large ,gradient descent will overshoot.If alpha is too low ,algorithm will work slowly and will take long time to converge.\n",
        "\n",
        "#**Loss Function**\n",
        "<br>\n",
        "\n",
        "* Mean Squared Error\n",
        "* Mean Absolute Error\n",
        "* R square\n",
        "\n",
        "**Mean Squared Error**<br>\n",
        "MSE is the sum of squared distances between our target variable and predicted values.<br>\n",
        "![](https://cdn-images-1.medium.com/max/1600/1*mlXnpXGdhMefPybSQtRmDA.png)\n",
        "\n",
        "**Mean Absolute Error**<br>\n",
        "Mean Absolute Error (MAE) is another loss function used for regression models. MAE is the sum of absolute differences between our target and predicted variables. So it measures the average magnitude of errors in a set of predictions, without considering their directions<br>\n",
        "![](https://cdn-images-1.medium.com/max/1600/1*xjarhfIDtRcaNhp7ZEyEdg.png)\n",
        "\n",
        "In short, using the squared error is easier to solve, but using the absolute error is more robust to outliers\n",
        "\n",
        "One big problem in using MAE loss (for neural nets especially) is that its gradient is the same throughout, which means the gradient will be large even for small loss values. This isn’t good for learning. To fix this, we can use dynamic learning rate which decreases as we move closer to the minima. MSE behaves nicely in this case and will converge even with a fixed learning rate. The gradient of MSE loss is high for larger loss values and decreases as loss approaches 0, making it more precise at the end of training (see figure below.)<br>\n",
        "![](https://cdn-images-1.medium.com/max/800/1*JTC4ReFwSeAt3kvTLq1YoA.png)<br>\n",
        "**R square**\n",
        "<br>\n",
        "\n",
        "R-squared evaluates the scatter of the data points around the fitted regression line. It is also called the coefficient of determination, or the coefficient of multiple determination for multiple regression. For the same data set, higher R-squared values represent smaller differences between the observed data and the fitted values.\n",
        "\n",
        "R-squared is the percentage of the dependent variable variation that a linear model explains.\n",
        "<br>\n",
        "\n",
        "![](https://s0.wp.com/latex.php?latex=%7B%5Cdisplaystyle+R%5E2+%3D+%5Cfrac+%7B%5Ctext%7BVariance+explained+by+the+model%7D%7D%7B%5Ctext%7BTotal+variance%7D%7D%7D&bg=ffffff&fg=000&s=0)<br>\n",
        "Formula of R-square<br>\n",
        "\n",
        "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/0ab5cc13b206a34cc713e153b192f93b685fa875)\n",
        "\n",
        "<br>\n",
        "The total sum of squares (proportional to the variance of the data)<br>\n",
        "\n",
        "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/aec2d91094ee54fbf0f7912d329706ff016ec1bd)<br>\n",
        "The regression sum of squares, also called the explained sum of squares:<br>\n",
        "\n",
        "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/107a9fb71364b9db3cf481e956ad2af11cba10a1)<br>\n",
        "\n",
        "The sum of squares of residuals, also called the residual sum of squares<br>\n",
        "\n",
        "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/2669c9340581d55b274d3b8ea67a7deb2225510b)<br>\n",
        "\n",
        "\n",
        "\n",
        "R-squared is always between 0 and 100%:\n",
        "\n",
        "* 0% represents a model that does not explain any of the variation in the response variable around its mean. The mean of the dependent variable predicts the dependent variable as well as the regression model.\n",
        "* 100% represents a model that explains all of the variation in the response variable around its mean.\n",
        "<br>\n",
        "\n",
        "R2 does not indicate whether:\n",
        "\n",
        "* the independent variables are a cause of the changes in the dependent variable\n",
        "* the most appropriate set of independent variables has been chosen\n",
        "<br>\n",
        "**Adjusted R-Square**<br>\n",
        "R-square assumes that every single variable explains the variation in the dependent variable. The adjusted R-square tells you the percentage of variation explained by only the independent variables that actually affect the dependent variable.<br>\n",
        "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/a7be77abfaf0007c245d9bf990593d474e7008e5)<br>\n",
        "where p is the total number of explanatory variables in the model (not including the constant term), and n is the sample size.\n",
        "<br>\n",
        "\n",
        "#Overfitting/Underfitting(Hyperparameter)\n",
        "\n",
        "Alpha is the hyperparameter of Linear regression<br>\n",
        "\n",
        "![](https://miro.medium.com/max/800/1*_7OPgojau8hkiPUiHoGK_w.png)\n",
        "<br>\n",
        "\n",
        "#**Overfiting**<br>\n",
        "When we train our model our motive is to learn from data rather than memorizing it.But sometimes our model start memorizing data.Because of this we get very low or no error on trainig data,but when use test data than it performs too bad.It happens because our model is overfitted.We can reduce this by making our model simpler.\n",
        "<br>\n",
        "**Ways to deal with overfitting**\n",
        "* Regularization<br>\n",
        "In these we reduce the number of features we are using in building our model or we can say that reducing the values of coeeficients.<br>\n",
        "\n",
        "In order to create less complex (parsimonious) model when you have a large number of features in your dataset, some of the Regularization techniques used to address over-fitting and feature selection are:\n",
        "\n",
        "1. L1 Regularization<br>\n",
        "\n",
        "2. L2 Regularization<br>\n",
        "\n",
        "**Ridge Regression**<br>\n",
        "A regression model that uses L1 regularization technique is called Lasso Regression and model which uses L2 is called Ridge Regression.<br>\n",
        "\n",
        "The key difference between these two is the penalty term.<br>\n",
        "![](https://cdn-images-1.medium.com/max/600/1*jgWOhDiGjVp-NCSPa5abmg.png)<br>\n",
        "Ridge regression adds “squared magnitude” of coefficient as penalty term to the loss function. Here the highlighted part represents L2 regularization element.<br>\n",
        "\n",
        "\n",
        "\n",
        "Cost function<br>\n",
        "Here, if lambda is zero then you can imagine we get back OLS. However, if lambda is very large then it will add too much weight and it will lead to under-fitting. Having said that it’s important how lambda is chosen. This technique works very well to avoid over-fitting issue.<br>\n",
        "\n",
        "**Lasso Regression**<br>\n",
        "Lasso Regression (Least Absolute Shrinkage and Selection Operator) adds “absolute value of magnitude” of coefficient as penalty term to the loss function.<br>\n",
        "![](https://cdn-images-1.medium.com/max/600/1*4MlW1d3xszVAGuXiJ1U6Fg.png)<br>\n",
        "\n",
        "Cost function<br>\n",
        "Again, if lambda is zero then we will get back OLS whereas very large value will make coefficients zero hence it will under-fit.\n",
        "\n",
        "*The key difference between these techniques is that Lasso shrinks the less important feature’s coefficient to zero thus, removing some feature altogether. So, this works well for feature selection in case we have a huge number of features.*\n",
        "\n",
        "\n",
        "\n",
        "**Underfiting**<br>\n",
        "In case of underfitting our model becomes dumb.In this case our model is not able to learn anything from data.Here whatever value we put in test data the output remains same.In simple way we can say that model is tooo simple.We can reduce this by making our model more complex.WE can use polynomial regression to handle underfiitting.If we have less amount of data then also underfitting occurs\n",
        "<br>\n",
        "\n",
        "\n",
        "#Continuous and Discrete Data\n",
        "![](http://dufei.club/wp-content/uploads/2018/07/examples-of-continuous-variables-math-continuous-and-discrete-data-mathematics.jpg)<br>\n",
        "\n",
        "\n",
        "\n",
        "#Encoding Categorical Data<br>\n",
        "![](https://i2.wp.com/www.real-statistics.com/wp-content/uploads/2015/03/categorical-data-coding.png?resize=635%2C367)<br>\n",
        "This is called Dummy encoding.When we have categorical data,we do this to convert them into digits.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cFRsz9O4ZfYh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import missingno as msno\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "######Root Mean Squared Error\n",
        "def error(x,y):\n",
        "    return np.sqrt(mean_squared_error(x,y))\n",
        "\n",
        "# X,y=make_regression(n_samples=100,n_features=1,random_state=0,noise=0,tail_strength=0,n_informative=1)\n",
        "# plt.figure(1)\n",
        "# plt.scatter(X,y)\n",
        "# plt.show()\n",
        "\n",
        "# xtrain,xtest,ytrain,ytest=train_test_split(X,y,test_size=0.3,random_state=0)\n",
        "\n",
        "# lr=LinearRegression()\n",
        "# lr.fit(xtrain,ytrain)\n",
        "# ypred=lr.predict(xtest)\n",
        "# plt.figure(2)\n",
        "# plt.plot(xtest,ypred,color='red')\n",
        "# plt.scatter(xtest,ytest,color='green')\n",
        "# print(error(ytest,ypred))\n",
        "\n",
        "# plt.figure(3)\n",
        "# sns.distplot(X)\n",
        "##########################################################################################################################################\n",
        "######################################################Overfitting/Underfitting#################################################################################\n",
        "######Ridge regression\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "##########################################################################################################################################\n",
        "########################now lets see performance when datapoints increases################################################################\n",
        "# error_data=[]\n",
        "# for i in range(100,3000,50):\n",
        "#     X,y=make_regression(n_samples=i,n_features=1,random_state=0,noise=0,tail_strength=0,n_informative=1)\n",
        "#     xtrain,xtest,ytrain,ytest=train_test_split(X,y,test_size=0.3,random_state=0)\n",
        "#     lr=LinearRegression()\n",
        "#     lr.fit(xtrain,ytrain)\n",
        "#     ypred=lr.predict(xtest)\n",
        "#     error_data.append(error(ytest,ypred))\n",
        "#     print(\"sample {} error {}\".format(i,error(ytest,ypred)))\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "##########################################################################################################################################\n",
        "##############################################large dimension#############################################################################\n",
        "# for i in range(10,100000,5000):\n",
        "#     X,y=make_regression(n_samples=100,n_features=i,random_state=0,noise=0,)\n",
        "#     xtrain,xtest,ytrain,ytest=train_test_split(X,y,test_size=0.3,random_state=0)\n",
        "#     lr=LinearRegression()\n",
        "#     lr.fit(xtrain,ytrain)   \n",
        "#     ypred=lr.predict(xtest)\n",
        "#     error_data.append(error(ytest,ypred))\n",
        "# plt.plot([i for i in range(10,100000,5000)],error_data)\n",
        "\n",
        "\n",
        "\n",
        "######## as the dimension increases performance decreases\n",
        "#197.66 100000\n",
        "#221 10000\n",
        "# 158.00 1000\n",
        "#107.32 100\n",
        "#3.46 10\n",
        " \n",
        "##########################################################################################################################################\n",
        "##########################################################missing values################################################################################\n",
        "# X,y=make_regression(n_samples=1000,n_features=1,random_state=0,noise=0.5)\n",
        "# xtrain,xtest,ytrain,ytest=train_test_split(X,y,test_size=0.3,random_state=0)\n",
        "# lr=LinearRegression()\n",
        "# lr.fit(xtrain,ytrain)\n",
        "# ypred=lr.predict(xtest)\n",
        "# print('Error before having missing values',error(ytest,ypred))\n",
        "# for i in range(5,700,10):\n",
        "#     X[i]=np.nan\n",
        "# print(len(np.argwhere(np.isnan(X)))) #70 missing values\n",
        "# #cant use directly ,we need to imputer values\n",
        "\n",
        "# #let's put zeroes\n",
        "# for i,j in enumerate(X):\n",
        "#     if np.isnan(j)==True:\n",
        "#         X[i]=0\n",
        "\n",
        "# xtrain,xtest,ytrain,ytest=train_test_split(X,y,test_size=0.3,random_state=0)\n",
        "# lr=LinearRegression()\n",
        "# lr.fit(xtrain,ytrain)\n",
        "# ypred=lr.predict(xtest)\n",
        "# print('Error after putting zero at missing values',error(ytest,ypred))\n",
        "\n",
        "# #let's put mean \n",
        "# for i,j in enumerate(X):\n",
        "#     if np.isnan(j)==True:\n",
        "#         X[i]=np.mean(X)\n",
        "\n",
        "# xtrain,xtest,ytrain,ytest=train_test_split(X,y,test_size=0.3,random_state=0)\n",
        "# lr=LinearRegression()\n",
        "# lr.fit(xtrain,ytrain)\n",
        "# ypred=lr.predict(xtest)\n",
        "# print(np.mean(X))\n",
        "# print('Error after putting Mean value at missing values',error(ytest,ypred))\n",
        "# #\n",
        "# for i,j in enumerate(X):\n",
        "#     if np.isnan(j)==True:\n",
        "#         X[i]=np.median(X)\n",
        "\n",
        "# xtrain,xtest,ytrain,ytest=train_test_split(X,y,test_size=0.3,random_state=0)\n",
        "# lr=LinearRegression()\n",
        "# lr.fit(xtrain,ytrain)\n",
        "# ypred=lr.predict(xtest)\n",
        "# print(np.median(X))\n",
        "# print('Error after putting median value at missing values',error(ytest,ypred))\n",
        "\n",
        "###########################################################################################################################################################\n",
        "#########################################################Imbalance Dataset#################################################################################\n",
        "# X,y=make_regression(n_samples=1000,n_features=1,random_state=0,noise=0.5,n_targets=2)\n",
        "# xtrain,xtest,ytrain,ytest=train_test_split(X,y,test_size=0.3,random_state=0)\n",
        "# lr=LinearRegression()\n",
        "# lr.fit(xtrain,ytrain)\n",
        "# ypred=lr.predict(xtest)\n",
        "# print('Error before imbalancing',error(ytest[:,[0]],ypred[:,[0]]))\n",
        "\n",
        "# plt.figure(1)\n",
        "# plt.title('First class of Y before imbalancing')\n",
        "# sns.distplot(y[:,[0]])\n",
        "\n",
        "# print('Number of zeros before imbalancing ',len([i for i in y[:,[0]] if i==0]))\n",
        "\n",
        "# for i in range(1,990):\n",
        "#     y[i,[0]]=0\n",
        "# print('Number of zeros after imbalancing',len([i for i in y[:,[0]] if i==0]))\n",
        "\n",
        "# plt.figure(2)\n",
        "# plt.title('First class of Y after imbalancing')\n",
        "# sns.distplot(y[:,[0]])\n",
        "\n",
        "# xtrain,xtest,ytrain,ytest=train_test_split(X,y,test_size=0.3,random_state=0)\n",
        "# lr=LinearRegression()\n",
        "# lr.fit(xtrain,ytrain)\n",
        "# ypred=lr.predict(xtest)\n",
        "\n",
        "# print('error after imputing zeros',error(ytest[:,[0]],ypred[:,[0]]))\n",
        "\n",
        "###########################################################################################################################################################\n",
        "#################################################Outliers##########################################################################################################\n",
        "# X,y=make_regression(n_samples=1000,n_features=1,random_state=0,noise=0.5)\n",
        "# xtrain,xtest,ytrain,ytest=train_test_split(X,y,test_size=0.3,random_state=0)\n",
        "# lr=LinearRegression()\n",
        "# lr.fit(xtrain,ytrain)\n",
        "# ypred=lr.predict(xtest)\n",
        "# print('Error before outliers',error(ytest,ypred))\n",
        "# plt.figure(1)\n",
        "# plt.title('X before Outliers')\n",
        "# plt.scatter(X,y)\n",
        "# # sns.distplot(X)\n",
        "# X[50]=500\n",
        "# X[89]=100\n",
        "# X[700]=900\n",
        "# X[34]=899\n",
        "# X[45]=2421\n",
        "# X[78]=3245\n",
        "# X[12]=1234\n",
        "# X[56]=234\n",
        "# X[123]=768\n",
        "# X[96]=234\n",
        "# plt.figure(2)\n",
        "# plt.title('X after outliers')\n",
        "# plt.scatter(X,y)\n",
        "\n",
        "# xtrain,xtest,ytrain,ytest=train_test_split(X,y,test_size=0.3,random_state=0)\n",
        "# lr=LinearRegression()\n",
        "# lr.fit(xtrain,ytrain)\n",
        "# ypred=lr.predict(xtest)\n",
        "# print('Error after outliers',error(ytest,ypred))\n",
        "\n",
        "###########################################################################################################################################################\n",
        "#################################################Discrete/Continuous features#####################################################################\n",
        "#till now we were using continuous features so we know linear regression workks well on them.\n",
        "#on categorical data we can use one hoe encoding to use linear regression\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kp3m-ZoYcWLn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "########################################gradient descent##########################################################################################################################################\n",
        "###################################################################################################################################################################################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "data=pd.read_csv('linear.csv')\n",
        "x=data.iloc[:,0:1]\n",
        "y=data.iloc[:,1:2]\n",
        "x=np.array(x)\n",
        "y=np.array(y)\n",
        "plt.figure(1)\n",
        "plt.scatter(x,y)\n",
        "# print(x.shape)\n",
        "# plt.scatter(x,y)\n",
        "m=np.array([0])\n",
        "x=np.transpose(x)\n",
        "m=m.reshape(len(m),1)\n",
        "b=np.array([0])\n",
        "b=b.reshape(len(b),1)\n",
        "# print(b.shape,m.shape,x.shape)\n",
        "n_iter=500\n",
        "alpha=0.0001\n",
        "error1=[]\n",
        "n=len(x)\n",
        "# print(x.shape)\n",
        "for i in range(n_iter):\n",
        "    ypred=np.dot(np.transpose(m),x)+b   \n",
        "#     print('iter {} ypred {}'.format(i,ypred))\n",
        "#     print('m {} b {} error {}'.format(m,b,1/n*(sum([i**2 for i in y-ypred]))))   \n",
        "    cost=(1/n*(sum([i**2 for i in y-ypred])))\n",
        "    error1.append(np.median(cost))\n",
        "    print('iteration {}\\t\\t\\terror {}'.format(i,np.median(cost)))\n",
        "    m_next=1/n*(sum((y-ypred)*x))\n",
        "    b_next=1/n*(sum((y-ypred)))\n",
        "    m=m-alpha*m_next\n",
        "    b=b-alpha*b_next\n",
        "#     print('At iteration {} error {}'.format(i,np.mean(1/n*(sum([i**2 for i in y-ypred])))))\n",
        "\n",
        "plt.figure(2)\n",
        "plt.plot([i for i in range(n_iter)],error1)    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1GQ0p1t0dfJS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}